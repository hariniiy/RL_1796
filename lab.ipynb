{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQQl3VCrA9+CP1AznNKsvb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariniiy/RL_1796/blob/main/lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dszp6Scxqdu",
        "outputId": "64f8c269-37de-4bbc-e6a2-bd476ba58fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Optimal State Values:\n",
            "V(s1) = 49.7802\n",
            "V(s2) = 44.1758\n",
            "\n",
            "ü§ñ Optimal Policy:\n",
            "œÄ(s1) = a1\n",
            "œÄ(s2) = a1\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(states, actions, transition_probs, rewards, gamma=0.9, theta=1e-6):\n",
        "\n",
        "    V = {s: 0.0 for s in states}\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            v = V[s]\n",
        "            action_values = []\n",
        "            for a in actions:\n",
        "                value = 0\n",
        "                for s_prime in states:\n",
        "                    prob = transition_probs[s][a].get(s_prime, 0)\n",
        "                    reward = rewards[s][a].get(s_prime, 0)\n",
        "                    value += prob * (reward + gamma * V[s_prime])\n",
        "                action_values.append(value)\n",
        "            V[s] = max(action_values)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Extract optimal policy\n",
        "    policy = {}\n",
        "    for s in states:\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        for a in actions:\n",
        "            value = 0\n",
        "            for s_prime in states:\n",
        "                prob = transition_probs[s][a].get(s_prime, 0)\n",
        "                reward = rewards[s][a].get(s_prime, 0)\n",
        "                value += prob * (reward + gamma * V[s_prime])\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "        policy[s] = best_action\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "\n",
        "\n",
        "states = ['s1', 's2']\n",
        "actions = ['a1', 'a2']\n",
        "\n",
        "# Transition probabilities: P[s][a][s']\n",
        "transition_probs = {\n",
        "    's1': {\n",
        "        'a1': {'s1': 0.5, 's2': 0.5},\n",
        "        'a2': {'s1': 0.7, 's2': 0.3}\n",
        "    },\n",
        "    's2': {\n",
        "        'a1': {'s1': 0.4, 's2': 0.6},\n",
        "        'a2': {'s1': 0.6, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Rewards: R[s][a][s']\n",
        "rewards = {\n",
        "    's1': {\n",
        "        'a1': {'s1': 5, 's2': 10},\n",
        "        'a2': {'s1': -1, 's2': 2}\n",
        "    },\n",
        "    's2': {\n",
        "        'a1': {'s1': 0, 's2': 4},\n",
        "        'a2': {'s1': 1, 's2': 1}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "V, policy = value_iteration(states, actions, transition_probs, rewards, gamma=0.9)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nüîç Optimal State Values:\")\n",
        "for s in V:\n",
        "    print(f\"V({s}) = {V[s]:.4f}\")\n",
        "\n",
        "print(\"\\nü§ñ Optimal Policy:\")\n",
        "for s in policy:\n",
        "    print(f\"œÄ({s}) = {policy[s]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrE8raKy4Igh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "class DummyEnv:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nA = 2  # 2 actions\n",
        "        self.states = [0, 1, 2, 3]\n",
        "        self.terminal = 3\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state == self.terminal:\n",
        "            return self.state, 0, True, {}\n",
        "        next_state = self.state + 1\n",
        "        reward = 1 if next_state == self.terminal else 0\n",
        "        done = next_state == self.terminal\n",
        "        self.state = next_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "\n",
        "\n",
        "def mc_policy_evaluation(env, policy, episodes=1000, gamma=0.9):\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(int)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited_states = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, _, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            if state not in visited_states:\n",
        "                returns_sum[state] += G\n",
        "                returns_count[state] += 1\n",
        "                V[state] = returns_sum[state] / returns_count[state]\n",
        "                visited_states.add(state)\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "def mc_control_epsilon_greedy(env, episodes=10000, gamma=0.9, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: defaultdict(float))\n",
        "    returns_sum = defaultdict(lambda: defaultdict(float))\n",
        "    returns_count = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def get_action(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice(range(env.nA))\n",
        "        else:\n",
        "            q_values = Q[state]\n",
        "            if not q_values:\n",
        "                return random.randint(0, env.nA - 1)\n",
        "            return max(q_values, key=q_values.get)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            if (state, action) not in visited:\n",
        "                returns_sum[state][action] += G\n",
        "                returns_count[state][action] += 1\n",
        "                Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
        "                visited.add((state, action))\n",
        "\n",
        "    # Extract policy\n",
        "    policy = {}\n",
        "    for state in Q:\n",
        "        policy[state] = max(Q[state], key=Q[state].get)\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = DummyEnv()\n",
        "\n",
        "    # Define a random policy for evaluation\n",
        "    def random_policy(state):\n",
        "        return random.choice([0, 1])\n",
        "\n",
        "    # Evaluate random policy\n",
        "    V = mc_policy_evaluation(env, random_policy, episodes=5000)\n",
        "    print(\"\\nüîç Monte Carlo Policy Evaluation (Random Policy):\")\n",
        "    for state in sorted(V):\n",
        "        print(f\"V({state}) = {V[state]:.4f}\")\n",
        "\n",
        "    # Find optimal policy using control\n",
        "    Q, learned_policy = mc_control_epsilon_greedy(env, episodes=10000, epsilon=0.1)\n",
        "    print(\"\\nü§ñ Monte Carlo Control (Œµ-greedy):\")\n",
        "    print(\"Learned Policy:\")\n",
        "    for state in sorted(learned_policy):\n",
        "        print(f\"œÄ({state}) = {learned_policy[state]}\")\n",
        "\n",
        "    print(\"\\nState-Action Values:\")\n",
        "    for state in sorted(Q):\n",
        "        for action in Q[state]:\n",
        "            print(f\"Q({state}, {action}) = {Q[state][action]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u70mc_zm1_gX",
        "outputId": "9c8ce18c-c43f-418d-a3fe-dddbcae3b981"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Monte Carlo Policy Evaluation (Random Policy):\n",
            "V(0) = 0.8100\n",
            "V(1) = 0.9000\n",
            "V(2) = 1.0000\n",
            "\n",
            "ü§ñ Monte Carlo Control (Œµ-greedy):\n",
            "Learned Policy:\n",
            "œÄ(0) = 1\n",
            "œÄ(1) = 0\n",
            "œÄ(2) = 0\n",
            "\n",
            "State-Action Values:\n",
            "Q(0, 0) = 0.8100\n",
            "Q(0, 1) = 0.8100\n",
            "Q(1, 1) = 0.9000\n",
            "Q(1, 0) = 0.9000\n",
            "Q(2, 0) = 1.0000\n",
            "Q(2, 1) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# =====================================\n",
        "# Simple GridWorld Environment\n",
        "# =====================================\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, width=4, height=4, terminal_states=[(0, 0), (3, 3)]):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.terminal_states = terminal_states\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (3, 0)  # Start at bottom-left\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.state in self.terminal_states:\n",
        "            return self.state, 0, True\n",
        "\n",
        "        x, y = self.state\n",
        "        if action == 'up': x = max(x - 1, 0)\n",
        "        elif action == 'down': x = min(x + 1, self.height - 1)\n",
        "        elif action == 'left': y = max(y - 1, 0)\n",
        "        elif action == 'right': y = min(y + 1, self.width - 1)\n",
        "\n",
        "        next_state = (x, y)\n",
        "        reward = 0 if next_state not in self.terminal_states else 1\n",
        "        done = next_state in self.terminal_states\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def get_all_states(self):\n",
        "        return [(i, j) for i in range(self.height) for j in range(self.width)]\n",
        "\n",
        "# =====================================\n",
        "# TD(0) Policy Evaluation\n",
        "# =====================================\n",
        "\n",
        "def td_0(env, policy, episodes=1000, alpha=0.1, gamma=0.9):\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
        "            state = next_state\n",
        "\n",
        "    return V\n",
        "\n",
        "# =====================================\n",
        "# SARSA Algorithm (Œµ-greedy)\n",
        "# =====================================\n",
        "\n",
        "def sarsa(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def choose_action(state):\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice(env.actions)\n",
        "        else:\n",
        "            q_vals = Q[state]\n",
        "            return max(q_vals, key=q_vals.get, default=random.choice(env.actions))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        action = choose_action(state)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_action = choose_action(next_state)\n",
        "\n",
        "            td_target = reward + gamma * Q[next_state][next_action]\n",
        "            Q[state][action] += alpha * (td_target - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    # Derive policy\n",
        "    policy = {}\n",
        "    for state in env.get_all_states():\n",
        "        if state in Q:\n",
        "            policy[state] = max(Q[state], key=Q[state].get)\n",
        "        else:\n",
        "            policy[state] = random.choice(env.actions)\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "# =====================================\n",
        "# Run & Display Results\n",
        "# =====================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = GridWorld()\n",
        "\n",
        "    # Define a random policy for TD(0)\n",
        "    def random_policy(state):\n",
        "        return random.choice(env.actions)\n",
        "\n",
        "    # Run TD(0) to evaluate the random policy\n",
        "    V = td_0(env, random_policy, episodes=5000)\n",
        "    print(\"\\nüîç TD(0) Value Function (Random Policy):\")\n",
        "    for i in range(env.height):\n",
        "        row = [f\"{V[(i, j)]:.2f}\" for j in range(env.width)]\n",
        "        print(\" \".join(row))\n",
        "\n",
        "    # Run SARSA to learn optimal policy\n",
        "    Q, learned_policy = sarsa(env, episodes=10000)\n",
        "\n",
        "    print(\"\\nü§ñ SARSA-Learned Policy:\")\n",
        "    for i in range(env.height):\n",
        "        row = [learned_policy[(i, j)][0].upper() if (i, j) in learned_policy else '.' for j in range(env.width)]\n",
        "        print(\" \".join(row))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3ikyn0H9N4X",
        "outputId": "27d86933-2011-4522-e0cf-53126d7941e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç TD(0) Value Function (Random Policy):\n",
            "0.00 0.51 0.32 0.26\n",
            "0.55 0.35 0.29 0.29\n",
            "0.41 0.31 0.35 0.38\n",
            "0.30 0.31 0.47 0.00\n",
            "\n",
            "ü§ñ SARSA-Learned Policy:\n",
            "D R D R\n",
            "U D L L\n",
            "U R D U\n",
            "R R R D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59nFPJFX-rBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def q_learning(env_name, episodes=10000, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, min_epsilon=0.01):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm for discrete action environments.\n",
        "\n",
        "    Parameters:\n",
        "        env_name: Gym environment name string\n",
        "        episodes: number of episodes to train\n",
        "        alpha: learning rate\n",
        "        gamma: discount factor\n",
        "        epsilon: exploration rate (epsilon-greedy)\n",
        "        epsilon_decay: decay rate of epsilon after each episode\n",
        "        min_epsilon: minimum epsilon\n",
        "\n",
        "    Returns:\n",
        "        Q-table and environment instance\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    state_space_size = env.observation_space.n\n",
        "    action_space_size = env.action_space.n\n",
        "\n",
        "    Q = np.zeros((state_space_size, action_space_size))\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            best_next_action = np.argmax(Q[next_state])\n",
        "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
        "            td_error = td_target - Q[state][action]\n",
        "            Q[state][action] += alpha * td_error\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "        if (episode + 1) % (episodes // 10) == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes} completed. Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return Q, env\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ENV_NAME = \"FrozenLake-v1\"  # Change to \"Taxi-v3\" if you want\n",
        "\n",
        "    Q, env = q_learning(ENV_NAME, episodes=10000)\n",
        "\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "\n",
        "    print(\"\\nLearned Q-table:\")\n",
        "    print(Q)\n",
        "\n",
        "    print(\"\\nDerived policy (best action per state):\")\n",
        "    print(policy)\n",
        "\n",
        "    # Test the learned policy for one episode\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    print(\"\\nRunning one episode with the learned policy:\")\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        print(f\"State: {state}, Action: {action}\")\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode finished with total reward: {total_reward}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XwHRzNb_TS4",
        "outputId": "5d69958f-5e3b-42c2-a4a0-ed5078be2b11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000/10000 completed. Epsilon: 0.3677\n",
            "Episode 2000/10000 completed. Epsilon: 0.1352\n",
            "Episode 3000/10000 completed. Epsilon: 0.0497\n",
            "Episode 4000/10000 completed. Epsilon: 0.0183\n",
            "Episode 5000/10000 completed. Epsilon: 0.0100\n",
            "Episode 6000/10000 completed. Epsilon: 0.0100\n",
            "Episode 7000/10000 completed. Epsilon: 0.0100\n",
            "Episode 8000/10000 completed. Epsilon: 0.0100\n",
            "Episode 9000/10000 completed. Epsilon: 0.0100\n",
            "Episode 10000/10000 completed. Epsilon: 0.0100\n",
            "\n",
            "Learned Q-table:\n",
            "[[0.53309112 0.48297452 0.48348996 0.47015764]\n",
            " [0.19383052 0.30900923 0.33957066 0.44431172]\n",
            " [0.37210476 0.24961985 0.24086319 0.24536764]\n",
            " [0.16504562 0.05263167 0.02733097 0.03461584]\n",
            " [0.54770963 0.38957774 0.3444187  0.41251059]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.29881392 0.08295876 0.15724911 0.09740837]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.36174316 0.37638384 0.35604983 0.57343738]\n",
            " [0.40347927 0.61974423 0.36467705 0.51059006]\n",
            " [0.61095118 0.36089928 0.29440335 0.28517969]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.50221015 0.57684248 0.72229317 0.56851892]\n",
            " [0.70596628 0.86138419 0.73813012 0.72720752]\n",
            " [0.         0.         0.         0.        ]]\n",
            "\n",
            "Derived policy (best action per state):\n",
            "[0 3 0 0 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "\n",
            "Running one episode with the learned policy:\n",
            "State: 0, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 0, Action: 0\n",
            "State: 0, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 0, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 8, Action: 3\n",
            "State: 4, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 8, Action: 3\n",
            "State: 4, Action: 0\n",
            "State: 4, Action: 0\n",
            "State: 8, Action: 3\n",
            "State: 4, Action: 0\n",
            "State: 8, Action: 3\n",
            "State: 9, Action: 1\n",
            "State: 13, Action: 2\n",
            "State: 14, Action: 1\n",
            "Episode finished with total reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "# =======================\n",
        "# Hyperparameters\n",
        "# =======================\n",
        "ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "EPISODES = 1000\n",
        "LEARNING_RATE = 1e-4\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_BUFFER_SIZE = 100_000\n",
        "MIN_REPLAY_SIZE = 10_000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.1\n",
        "EPSILON_DECAY = 1_000_000\n",
        "TARGET_UPDATE_FREQ = 10_000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =======================\n",
        "# Preprocessing Wrappers\n",
        "# =======================\n",
        "\n",
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0], shp[1], k), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, _ = self.env.reset(**kwargs)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return np.concatenate(self.frames, axis=-1), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return np.concatenate(self.frames, axis=-1), reward, done, truncated, info\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name, render_mode=None)\n",
        "    env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=True, scale_obs=False, frame_skip=4)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    return env\n",
        "\n",
        "# =======================\n",
        "# Q-Network\n",
        "# =======================\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x / 255.0)\n",
        "\n",
        "# =======================\n",
        "# Experience Replay Buffer\n",
        "# =======================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, dones, next_states = zip(*batch)\n",
        "        return (\n",
        "            np.stack(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(dones),\n",
        "            np.stack(next_states)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# =======================\n",
        "# Training\n",
        "# =======================\n",
        "\n",
        "def train():\n",
        "    env = make_env(ENV_NAME)\n",
        "    obs_shape = env.observation_space.shape\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    online_net = DQN((obs_shape[2], obs_shape[0], obs_shape[1]), n_actions).to(DEVICE)\n",
        "    target_net = DQN((obs_shape[2], obs_shape[0], obs_shape[1]), n_actions).to(DEVICE)\n",
        "    target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
        "    replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    epsilon = EPSILON_START\n",
        "    total_steps = 0\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        state, _ = env.reset()\n",
        "        state = np.transpose(state, (2, 0, 1))  # CHW\n",
        "        done = False\n",
        "        truncated = False\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not done and not truncated:\n",
        "            total_steps += 1\n",
        "            epsilon = max(EPSILON_END, EPSILON_START - total_steps / EPSILON_DECAY)\n",
        "\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.tensor([state], dtype=torch.float32, device=DEVICE)\n",
        "                    q_values = online_net(state_tensor)\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            next_state_transposed = np.transpose(next_state, (2, 0, 1))  # CHW\n",
        "\n",
        "            replay_buffer.push((state, action, reward, done, next_state_transposed))\n",
        "            state = next_state_transposed\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Train\n",
        "            if len(replay_buffer) >= MIN_REPLAY_SIZE:\n",
        "                states, actions, rewards, dones, next_states = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "                states_t = torch.tensor(states, dtype=torch.float32, device=DEVICE)\n",
        "                actions_t = torch.tensor(actions, dtype=torch.int64, device=DEVICE)\n",
        "                rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
        "                dones_t = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
        "                next_states_t = torch.tensor(next_states, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "                q_values = online_net(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
        "                with torch.no_grad():\n",
        "                    next_q_values = target_net(next_states_t).max(1)[0]\n",
        "                    target_q = rewards_t + GAMMA * next_q_values * (1 - dones_t)\n",
        "\n",
        "                loss = nn.functional.mse_loss(q_values, target_q)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update target network\n",
        "                if total_steps % TARGET_UPDATE_FREQ == 0:\n",
        "                    target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "        print(f\"Episode {episode + 1}, Reward: {episode_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# =======================\n",
        "# Run Training\n",
        "# =======================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    train()\n",
        "    print(f\"\\nTraining completed in {time.time() - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "YcvPYPYVAwy3",
        "outputId": "34029f05-dbbe-4275-f7a5-65d019a1ba21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameNotFound",
          "evalue": "Environment `PongNoFrameskip` doesn't exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3510753123.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining completed in {time.time() - start_time:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3510753123.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mobs_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3510753123.py\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtariPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFrameStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         raise error.Error(\n\u001b[1;32m    528\u001b[0m             \u001b[0;34mf\"No registered env with id: {env_name}. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\" Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;34mf\"Environment `{name}` doesn't exist{namespace_msg}.{suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     )\n",
            "\u001b[0;31mNameNotFound\u001b[0m: Environment `PongNoFrameskip` doesn't exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xv9OU6qeA-uM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}