{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMovicxTftV1WKfdO1mWeyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariniiy/RL_1796/blob/main/Untitled34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iep2fxF_IgcD",
        "outputId": "c6c845e1-2781-4a04-ecef-478eba80b942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training on FrozenLake-v1 (deterministic) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FrozenLake: Testing learned policy\n",
            "Episode reward: 1.0\n",
            "Episode reward: 1.0\n",
            "Episode reward: 1.0\n",
            "\n",
            "=== Training on Taxi-v3 ===\n",
            "\n",
            "Taxi-v3: Testing learned policy\n",
            "Episode reward: 20\n",
            "Episode reward: 20\n",
            "Episode reward: 20\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "\n",
        "def q_learning(env, name=\"Env\", episodes=5000, max_steps=100,\n",
        "               alpha=0.8, gamma=0.95,\n",
        "               epsilon=1.0, min_epsilon=0.01, decay_rate=0.005,\n",
        "               render=False):\n",
        "    \"\"\"\n",
        "    Q-learning for discrete-action Gymnasium environments.\n",
        "    Handles updated API (reset returns tuple, step returns terminated/truncated).\n",
        "    \"\"\"\n",
        "    state_size = env.observation_space.n\n",
        "    action_size = env.action_space.n\n",
        "    Q = np.zeros((state_size, action_size))\n",
        "\n",
        "    def get_epsilon(ep):\n",
        "        return min_epsilon + (epsilon - min_epsilon) * np.exp(-decay_rate * ep)\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        eps = get_epsilon(ep)\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            if random.random() < eps:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Q-learning update\n",
        "            Q[state, action] += alpha * (\n",
        "                reward + gamma * np.max(Q[new_state]) - Q[state, action]\n",
        "            )\n",
        "            total_reward += reward\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Optionally, print episode-by-episode reward\n",
        "        # if (ep + 1) % (episodes // 5) == 0:\n",
        "        #     print(f\"{name}: Episode {ep+1}/{episodes}, reward: {total_reward:.2f}\")\n",
        "\n",
        "    if render:\n",
        "        print(f\"\\n{name}: Testing learned policy\")\n",
        "        for _ in range(3):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            env.render()\n",
        "            while not done:\n",
        "                action = np.argmax(Q[state])\n",
        "                state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                env.render()\n",
        "            print(\"Episode reward:\", reward)\n",
        "    return Q\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # FrozenLake (deterministic)\n",
        "    print(\"=== Training on FrozenLake-v1 (deterministic) ===\")\n",
        "    env1 = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "    Q1 = q_learning(env1, name=\"FrozenLake\", episodes=10000, alpha=0.7, gamma=0.95,\n",
        "                    epsilon=1.0, min_epsilon=0.01, decay_rate=0.001, render=True)\n",
        "    env1.close()\n",
        "\n",
        "    # Taxi-v3\n",
        "    print(\"\\n=== Training on Taxi-v3 ===\")\n",
        "    env2 = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n",
        "    Q2 = q_learning(env2, name=\"Taxi-v3\", episodes=20000, alpha=0.9, gamma=0.95,\n",
        "                    epsilon=1.0, min_epsilon=0.01, decay_rate=0.0005, render=True)\n",
        "    env2.close()\n",
        "\n",
        ""
      ]
    }
  ]
}