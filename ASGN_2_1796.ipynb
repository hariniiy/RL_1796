{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariniiy/RL_1796/blob/main/ASGN_2_1796.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-uIZkCtR3WM",
        "outputId": "bdb5005c-2064-449f-ed50-4860d0c182e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Policy Evaluation (state values for random policy):\n",
            "{(18, 2, 0): -0.33640552995391704, (9, 2, 0): -0.3010033444816054, (15, 4, 1): -0.2727272727272727, (20, 10, 0): -0.2345999527967902, (10, 5, 0): -0.13910761154855644}\n",
            "\n",
            "Monte Carlo Control (optimal policy sample):\n",
            "State: (20, 9, 0), Best Action: 0\n",
            "State: (16, 10, 0), Best Action: 1\n",
            "State: (13, 10, 1), Best Action: 1\n",
            "State: (20, 6, 0), Best Action: 0\n",
            "State: (15, 10, 0), Best Action: 1\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------------------------\n",
        "# Monte Carlo Policy Evaluation\n",
        "# ---------------------------\n",
        "def mc_policy_evaluation(policy, env, num_episodes=500000, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Evaluate a given policy using first-visit Monte Carlo.\n",
        "    \"\"\"\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            done = terminated or truncated\n",
        "\n",
        "        # Compute returns\n",
        "        G = 0\n",
        "        visited_states = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            if state not in visited_states:\n",
        "                returns_sum[state] += G\n",
        "                returns_count[state] += 1\n",
        "                V[state] = returns_sum[state] / returns_count[state]\n",
        "                visited_states.add(state)\n",
        "\n",
        "    return V\n",
        "\n",
        "# ---------------------------\n",
        "# Monte Carlo Control (ε-greedy)\n",
        "# ---------------------------\n",
        "def mc_control_epsilon_greedy(env, num_episodes=500000, gamma=1.0, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Monte Carlo control using epsilon-greedy policy improvement.\n",
        "    \"\"\"\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    def policy_fn(state):\n",
        "        # ε-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(env.action_space.n)\n",
        "        else:\n",
        "            return np.argmax(Q[state])\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        episode = []\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy_fn(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            done = terminated or truncated\n",
        "\n",
        "        # Compute returns\n",
        "        G = 0\n",
        "        visited_state_actions = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = gamma * G + reward\n",
        "            if (state, action) not in visited_state_actions:\n",
        "                returns_sum[(state, action)] += G\n",
        "                returns_count[(state, action)] += 1\n",
        "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
        "                visited_state_actions.add((state, action))\n",
        "\n",
        "    # Derive final greedy policy\n",
        "    policy = {}\n",
        "    for state in Q:\n",
        "        policy[state] = np.argmax(Q[state])\n",
        "\n",
        "    return policy, Q\n",
        "\n",
        "# ---------------------------\n",
        "# Run Example with Blackjack\n",
        "# ---------------------------\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "\n",
        "# Define a random policy for evaluation\n",
        "def random_policy(state):\n",
        "    return np.random.choice(env.action_space.n)\n",
        "\n",
        "# Monte Carlo Policy Evaluation\n",
        "V = mc_policy_evaluation(random_policy, env, num_episodes=100000)\n",
        "print(\"Monte Carlo Policy Evaluation (state values for random policy):\")\n",
        "print(dict(list(V.items())[:5]))  # print sample\n",
        "\n",
        "# Monte Carlo Control\n",
        "optimal_policy, Q = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)\n",
        "print(\"\\nMonte Carlo Control (optimal policy sample):\")\n",
        "for k, v in list(optimal_policy.items())[:5]:\n",
        "    print(f\"State: {k}, Best Action: {v}\")\n"
      ]
    }
  ]
}